{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e372c41-beb6-49c3-a00e-8bd938fa354a",
   "metadata": {},
   "source": [
    "# How to select an objective function using information theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc99a9-9a6c-48ff-bfcf-4c5c30b3a04d",
   "metadata": {},
   "source": [
    "### Timothy O. Hodson<sup>1</sup>, Tyler J. Smith<sup>2</sup>, Lucy M. Marshall<sup>3</sup>, and Thomas M. Over<sup>1</sup>\n",
    "    \n",
    "<sup>1</sup> U.S. Geological Survey Central Midwest Water Science Center, Urbana, Illinois; <sup>2</sup> Clarkson University, Potsdam, New York; <sup>3</sup> University of New South Wales, Sydney\n",
    "\n",
    "[![GitHub tag (latest by date)](https://img.shields.io/github/v/tag/hytest-org/workflow-hodson-2022-objective-benchmark)](https://github.com)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hytest-org/workflow-hodson-2022-objective-benchmark/blob/main/01-objective-benchmark-demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c615c-4dfa-425c-bc6a-6a1ed0c4482a",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "Science tests competing theories or models by evaluating the similarity of their predictions against observational experience. \n",
    "Thus, how we measure similarity fundamentally determines what we learn. \n",
    "In modeling and machine learning, similarity metrics are used as objective functions.\n",
    "A classic example being mean squared error, which is the optimal measure of similarity when errors are normally distributed and IID. \n",
    "In many cases, however, the error distribution is neither normal nor IID, so it is left to the scientist to determine an appropriate objective.\n",
    "Here, we review how information theory can guide that selection, then apply the approach with a simple hydrologic model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8923947-ed9f-4e24-9ff7-dc14f04fd32d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "Science seeks to create useful representations of reality in the from of theories or hypotheses or models.\n",
    "What sets science apart from other pursuits is that it rigorously tests those representations against observational experience:\n",
    "all other things being equal, favor that which \"best\" fits the evidence.\n",
    "An analogous process occurs when calibrating a numerical model or evaluating among competing models.\n",
    "To select the \"best\" model, experiment by varying the model while keeping the test data and objective fixed.\n",
    "If mean squared error (MSE) is the objective, compute the MSE between the test data and the model predictions, then select the model with the lowest MSE.\n",
    "But why choose MSE and not another objective function?\n",
    "The answer: MSE is the optimal measure when errors are normally distributed and IID.\n",
    "But for many problems, the true error distribution is complex or unknown.\n",
    "\n",
    "To select the \"best\" objective for those cases, the experiment is essentially the same except the objective is varied while the model and data are held fixed.\n",
    "Now, select the objective indicating the greatest similarity between data and model.\n",
    "Different objectives have different scales, however, so they are ``normalized''\n",
    "such that each integrates to one; thereby representing them as probability distributions.\n",
    "The normalized form of MSE is the normal distribution, for example.\n",
    "When used to evaluate model fit, the probability distribution is called a likelihood function\n",
    "and its output the likelihood\n",
    "(for additional review, see Hodson, 2022).\n",
    "So, to select among objectives, compute their likelihoods, then favor the most likely.\n",
    "The convention is to work with the natural logarithm of the likelihood, denoted as $\\ell$.\n",
    "The logarithm does not change how the models rank\n",
    "but simplifies the math by converting products to sums.\n",
    "Likelihoods multiply, so log likelihoods add.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8349ec1-8d4d-43da-9239-fe639a4169e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Uncertainty to information\n",
    "Thus far, the selection problem is posed in terms of probability theory,\n",
    "but information theory gives an equally valid interpretation.\n",
    "The former seeks the most likely representation, whereas the latter seeks that with the best compression\n",
    "(Cover, 2006).\n",
    "Their equivalence is shown later,\n",
    "but first, we review three fundamental concepts from information theory.\n",
    "The entropy $H(D)$, which is the expected information in each new observation of the data $D$.\n",
    "The conditional entropy $ H(D|M)$,\n",
    "which is the additional information need to represented the $D$ when encoded with some model $M$\n",
    "(think of it as the information in the model error).\n",
    "Finally, the difference between these terms gives the mutual information\n",
    "$$\n",
    "I(D; M) = H(D) - H(D | M) \\text{,}\n",
    "$$\n",
    "which measures how much information $M$ encodes about $D$.\n",
    "The \"best\" model or, in this case, objective maximizes $I$.\n",
    "Hereafter, model and objectively are used interchangeably,\n",
    "but in a formal sense, the objective is a model representing the error, or information loss, between $M$ and $D$.\n",
    "\n",
    "If the data are fixed, the information in the data, $H(D)$, is constant $C$.\n",
    "Now, the connection to probability theory:\n",
    "as the number of observations $n$ goes to infinity,\n",
    "the average log likelihood equals the negative conditional entropy equals the mutual information up to a constant,\n",
    "$$\n",
    "I_C = - H(D|M) = \\ell / n \\text{,}\n",
    "$$\n",
    "where the natural logarithm gives units of nats.\n",
    "Dividing by $\\ln(2)$ converts the result to bits.\n",
    "In other words, the most likely objective function represents the error with the fewest bits.\n",
    "For finite $n$, the average $\\ell$ still gives an unbiased estimate of the conditional entropy,\n",
    "when assessed \"out of sample,\"\n",
    "meaning the test data were not used to calibrate the model.\n",
    "\n",
    "TODO\n",
    "So to benchmark an objective, use the log likelihood to estimate the information content of the model error.\n",
    "Log likelihoods for several objectives are implemented below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ef72a63-4417-4cb0-beea-260bb1b80c42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute likelihood\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "def normal_ll(y, y_hat, transform=None, gradient=1):\n",
    "    '''Log likelihood for the normal distribution with change of variable\n",
    "    \n",
    "    The normal distribution is the formal likelihood for the mean squared error (MSE).\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "        Observations.\n",
    "    y_hat : array_like\n",
    "        Predictions.\n",
    "    transform : function\n",
    "        Change of variable transformation.\n",
    "    gradient : function\n",
    "        Gradient of the transform function.\n",
    "        \n",
    "    Proof\n",
    "    -----\n",
    "    https://www.statlect.com/probability-distributions/normal-distribution\n",
    "    '''\n",
    "    if transform is not None:\n",
    "        y = transform(y)\n",
    "        y_hat = transform(y_hat)\n",
    "        \n",
    "    e = y - y_hat\n",
    "    n = len(e)\n",
    "    sigma = e.std()\n",
    "    log_gradient = np.sum(np.log(np.abs(gradient)))\n",
    "    ll = -n * np.log(sigma) - n/2*np.log(2*np.pi) - 1/(2*sigma**2) * (e**2).sum() + log_gradient\n",
    "    return ll\n",
    "\n",
    "\n",
    "def laplace_ll(y, y_hat, transform=None, gradient=1):\n",
    "    '''Log likelihood for Laplace distribution with change of variable\n",
    "    \n",
    "    The laplace distribution is the formal likelihood for the mean absolute\n",
    "    error (MAE).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "        Observations.\n",
    "    y_hat : array_like\n",
    "        Predictions.\n",
    "    transform : function\n",
    "        Change of variable transformation.\n",
    "    gradient : function\n",
    "        Gradient of the transform function.\n",
    "    '''\n",
    "    if transform is not None:\n",
    "        y = transform(y)\n",
    "        y_hat = transform(y_hat)\n",
    "        \n",
    "    e = (y - y_hat).abs()\n",
    "    n = len(e)\n",
    "    b = e.mean()\n",
    "    log_gradient = np.sum(np.log(np.abs(gradient)))\n",
    "    ll = -n * np.log(2*b) - 1/b * e.sum() + log_gradient\n",
    "    return ll.sum()\n",
    "                                   \n",
    "\n",
    "def msre_ll(y, y_hat):\n",
    "    '''Log likelihood for mean squared square-root error\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    '''\n",
    "    return normal_ll(y, y_hat, transform=lambda x: np.sqrt(x), gradient=-1/(2*np.sqrt(y)))\n",
    "\n",
    "\n",
    "def mare_ll(y, y_hat):\n",
    "    '''Log likelihood for mean absolute square-root error\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    '''\n",
    "    return laplace_ll(y, y_hat, transform=lambda x: np.sqrt(x), gradient=-1/(2*np.sqrt(y)))\n",
    "\n",
    "\n",
    "def lognormal_ll(y, y_hat):\n",
    "    '''Lognormal log likelihood\n",
    "    \n",
    "    The lognormal distribution is the formal likelihood for the mean squared\n",
    "    log error (MSLE).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    '''\n",
    "    return normal_ll(y, y_hat, transform=lambda x: np.log(x), gradient=1/y)\n",
    "\n",
    "\n",
    "def mspe_ll(y, y_hat):\n",
    "    '''Log likelhood for mean squared percentage error\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \n",
    "    '''\n",
    "    #return normal_ll(y, y_hat, transform=lambda x: x/y, gradient=1/np.abs(y))\n",
    "    return normal_ll(y, y_hat, transform=lambda x: x/y, gradient=-1/(y**2)) \n",
    "\n",
    "\n",
    "def nse_ll(y, y_hat, group='gage_id'):\n",
    "    '''Log likelihood for normalized squared error (NSE)\n",
    "    \n",
    "    NSE is equivalent to the Nash–Sutcliffe model efficiency coefficient.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    '''\n",
    "    sigma_o = y.groupby('gage_id').transform(lambda x: x.std())\n",
    "    return normal_ll(y, y_hat, transform=lambda x: x/sigma_o, gradient=1/sigma_o)\n",
    "\n",
    "\n",
    "def loglaplace_ll(y, y_hat):\n",
    "    '''Log likelihood for log Laplace distribution\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    '''\n",
    "    return laplace_ll(y, y_hat, transform=lambda x: np.log(x), gradient=1/y)\n",
    "\n",
    "\n",
    "def uniform_ll(y, y_hat):\n",
    "    '''Log likelihood for uniform distribution.\n",
    "    \n",
    "    The uniform log likelihood minimizes the maximum error.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    '''\n",
    "    e = np.abs(y - y_hat)\n",
    "    n = len(e)\n",
    "    #ll = -n * np.log(e.max()-e.min()) # standard formulation\n",
    "    ll = -n * np.log(e.max() - 0)\n",
    "    return ll\n",
    "\n",
    "\n",
    "def bernoulli_ll(y, y_hat, groupby=None):\n",
    "    '''TODO and use within zi_ll\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def zi_ll(y, y_hat, ll=normal_ll, threshold=0.01, groupby=None):\n",
    "    ''' Zero-inflated log likelihood.\n",
    "    \n",
    "     Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    ll : function\n",
    "        Zero-inflated log likelihood \n",
    "    threshold : float\n",
    "        Value below which is treated as zero\n",
    "    groupby : string\n",
    "        Optional groupby term (testing)\n",
    "    '''\n",
    "    y_o = y <= threshold\n",
    "    y_hat_o = y_hat <= threshold\n",
    "    \n",
    "    if groupby is None:\n",
    "        n1 = (y_o & y_hat_o).sum() # correct zero-flow prediction\n",
    "        n2 = (y_o ^ y_hat_o).sum() # incorrect zero-flow prediction \n",
    "    else:\n",
    "        n1 = (y_o & y_hat_o).groupby(groupby).sum() # correct zero-flow prediction\n",
    "        n2 = (y_o ^ y_hat_o).groupby(groupby).sum() # incorrect zero-flow prediction\n",
    "\n",
    "    n3 = (~y_o & ~y_hat_o) # correct flow predictions\n",
    "    \n",
    "    # fraction of correctly predicted zero flows\n",
    "    rho = np.where( (n1+n2) == 0, 0, n1 / (n1 + n2))\n",
    "    n_rho = 1-rho\n",
    "    \n",
    "    # n1 * np.log(rho) + n2 * np.log(1-rho)\n",
    "    ll_zero = n1[rho!=0] * np.log(rho[rho!=0]) + n2[n_rho!=0]* np.log(n_rho[n_rho!=0])\n",
    "    \n",
    "    return ll_zero.sum() + ll(y[n3], y_hat[n3])\n",
    "\n",
    "\n",
    "def zilognormal_ll(y, y_hat):\n",
    "    '''Log likelihood for zero-inflated lognormal.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    '''\n",
    "       \n",
    "    return zi_ll(y, y_hat, ll=lognormal_ll, threshold=0.01)\n",
    "\n",
    "\n",
    "def ziloglaplace_ll(y, y_hat):\n",
    "    '''Log likelihood for zero-inflated laplace.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    '''\n",
    "    return zi_ll(y, y_hat, ll=loglaplace_ll, threshold=0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165d83af-4345-4235-9ff8-b0a417a418b6",
   "metadata": {},
   "source": [
    "## Weights\n",
    "\n",
    "Given the conditional entropies for a set of $m$ models,\n",
    "the \"weight\" of evidence for each model $w_i$ is\n",
    "$$\n",
    "w_i = \\frac{ x^{\\hat H_i} }{ \\sum^{m}_{i} x^{\\hat H_i}  }\n",
    "$$\n",
    "where the base $x$ is 2 for bits or $e$ for nats (Burnham and Anderson, 2002)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0adaa4c8-de06-489f-a0fb-4d2b01c9c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights(series, base=np.e):\n",
    "    '''Compute posterior weights\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series : array_like\n",
    "        Log likelihoods\n",
    "    base: float\n",
    "        Base of the logarithm used to compute log likelihood\n",
    "    '''\n",
    "    s = base**series\n",
    "    return s/s.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6eb8b6-99f9-419b-a1b0-29e80b756ac5",
   "metadata": {},
   "source": [
    "## Benchmark demonstration\n",
    "As demonstration, we benchmark the entropies of several objective functions used with streamflow models.\n",
    "The test data are streamflow observations from 1,385 streamgages in the conterminous U.S. (Russell and others, 2020);\n",
    "roughly 14 million observations.\n",
    "Streamflow can be zero or negative, which is undefined for certain objectives, like those with log transformations.\n",
    "For the demonstration, flows below 0.0028 cubic meters per second (0.01 cfs) were thresholded and treated as the \"zero flow\" state.\n",
    "Different thresholds may yield slightly different results,\n",
    "particularly for objectives that are more sensitive at low flow, like those using logs.\n",
    "\n",
    "The model is simple: \n",
    "predict streamflow at a particular time and location by scaling the nearest observation by the ratio of the two drainage areas.\n",
    "So when predicting flow in a large river using observations from a smaller one, scale up the observations.\n",
    "By nature, the predictions are out of sample, so neither cross validation nor bias adjustment is necessary. \n",
    "Though the demonstration is almost trivial, the same procedure can be applied with machine learning or physics-based simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20f31ab5-6051-4145-ab0d-6507bbe868b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "ds = xr.open_dataset('GAGESII_reference_NNDAR.nc')\n",
    "df = ds.to_dataframe()\n",
    "df = df.dropna(how='any')\n",
    "#df = pd.read_parquet()\n",
    "df[df < 0.01] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d17876b7-aec1-48e6-86d5-94c6ce788768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: create a table of objective functions\n",
    "objectives = {\n",
    "    'U' : {'name':'uniformly distributed error', 'f':uniform_ll},\n",
    "    'MSE' : {'name':'mean squared error', 'f':normal_ll},\n",
    "    'NSE' : {'name':'normalized squared error', 'f':nse_ll},\n",
    "    'MAE' : {'name': 'mean absolute error', 'f':laplace_ll},\n",
    "    'MSPE' : {'name': 'mean squared percent error', 'f':mspe_ll},\n",
    "    'MSLE' : {'name':'mean squared log error*', 'f':lognormal_ll},\n",
    "    'MALE' : {'name':'mean absolute log error*', 'f':loglaplace_ll},\n",
    "    'ZMSLE' : {'name':'zero-inflated MSLE', 'f':zilognormal_ll},\n",
    "    'ZMALE' : {'name':'zero-inflated MALE', 'f':ziloglaplace_ll},\n",
    "    'MARE' : {'name':'mean absolute square root error', 'f':mare_ll},\n",
    "}\n",
    "\n",
    "obj_df = pd.DataFrame.from_dict(objectives, orient='index')\n",
    "\n",
    "# step 2: compute the information in each objective function\n",
    "for index, row in obj_df.iterrows():\n",
    "    # nats is the negative log likelihood or the info in the error\n",
    "    obj_df.loc[index, 'bits'] = - row.f(df.obs, df.NNDAR)/len(df)/np.log(2)\n",
    "\n",
    "# step 3: compute weights\n",
    "obj_df['weight'] = compute_weights(-obj_df.bits, base=2)\n",
    "\n",
    "# step 4: format output table\n",
    "\n",
    "table = obj_df[['name','bits','weight']].sort_values('weight').round(2)#.rename(columns=names)\n",
    "\n",
    "table['rank'] = len(table) - np.argsort(table['weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19532f6-cf72-4adf-96fa-d98aa34d5f21",
   "metadata": {},
   "source": [
    "Results are shown in the table below.\n",
    "The best objective represents the error ($\\hat H$) in the fewest bits.\n",
    "For this particular benchmark, zero-mean absolute log error (ZMALE) performed best,\n",
    "with an entropy of 6.95 bits.\n",
    "For comparison, MSE was 11.62 bits.\n",
    "The magnitudes are unimportant here, only the differences.\n",
    "The data and model were fixed;\n",
    "all that changed was how we quantified it.\n",
    "The real information content is unchanged.\n",
    "Relative to ZMALE, the excess bits in the other objectives are noise.\n",
    "So, MSE measures at least 40 percent noise,\n",
    "and NSE, the de facto in hydrologic modeling, at least 38 percent.\n",
    "Noiser objectives result in models that require more iterations of calibration, more data, and more storage space\n",
    "(better model, better data compression).\n",
    "A well-known example being stochastic gradient descent, where the noisier gradient requires more iterations to reach the solution.\n",
    "In that case, each iteration completes faster, so the solution can be reached quicker overall.\n",
    "A poorly chosen objective incurs a similar penalty but without a benefit.\n",
    "\n",
    "Despite their near ubiquituous use as a basis for learning,\n",
    "objective functions are rarely, if ever, benchmarked in many problem domains.\n",
    "Hopefully, by providing this simple demonstration the practice becomes commonplace.\n",
    "After all, how well machines---and scientists---learn and think can depend a lot on how they measure similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a920adc-2381-4d72-bc8b-54024a412374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>bits</th>\n",
       "      <th>weight</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSPE</th>\n",
       "      <td>mean squared percent error</td>\n",
       "      <td>23.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>U</th>\n",
       "      <td>uniformly distributed error</td>\n",
       "      <td>18.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>mean squared error</td>\n",
       "      <td>11.62</td>\n",
       "      <td>0.01</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NSE</th>\n",
       "      <td>normalized squared error</td>\n",
       "      <td>11.20</td>\n",
       "      <td>0.01</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>mean absolute error</td>\n",
       "      <td>9.49</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSLE</th>\n",
       "      <td>mean squared log error*</td>\n",
       "      <td>7.47</td>\n",
       "      <td>0.15</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARE</th>\n",
       "      <td>mean absolute square root error</td>\n",
       "      <td>7.34</td>\n",
       "      <td>0.17</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZMSLE</th>\n",
       "      <td>zero-inflated MSLE</td>\n",
       "      <td>7.18</td>\n",
       "      <td>0.19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MALE</th>\n",
       "      <td>mean absolute log error*</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZMALE</th>\n",
       "      <td>zero-inflated MALE</td>\n",
       "      <td>6.95</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name   bits  weight  rank\n",
       "MSPE        mean squared percent error  23.54    0.00    10\n",
       "U          uniformly distributed error  18.17    0.00     9\n",
       "MSE                 mean squared error  11.62    0.01     8\n",
       "NSE           normalized squared error  11.20    0.01     7\n",
       "MAE                mean absolute error   9.49    0.04     6\n",
       "MSLE           mean squared log error*   7.47    0.15     5\n",
       "MARE   mean absolute square root error   7.34    0.17     4\n",
       "ZMSLE               zero-inflated MSLE   7.18    0.19     3\n",
       "MALE          mean absolute log error*   7.04    0.21     2\n",
       "ZMALE               zero-inflated MALE   6.95    0.22     1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(table.to_latex())\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "def8d189-0817-4e9e-aa06-c5ce4928da5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is at least 40 percent noise.\n"
     ]
    }
   ],
   "source": [
    "# compute minimum noise is MSE\n",
    "mse_bits = table.loc['MSE', 'bits']\n",
    "nse_bits = table.loc['NSE', 'bits']\n",
    "zmale_bits = table.loc['ZMALE', 'bits']\n",
    "print(f'MSE is at least {round(100*(mse_bits - zmale_bits) / mse_bits)} percent noise.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4e67aed-713e-43e5-aa85-f209ebd23358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSE is at least 38 percent noise.\n"
     ]
    }
   ],
   "source": [
    "# compute minimum noise in NSE\n",
    "print(f'and NSE at least {round(100*(nse_bits - zmale_bits) / nse_bits)} percent.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d8534-2034-49a6-8083-8e765642b23f",
   "metadata": {},
   "source": [
    "## Data availability\n",
    "The streamflow data are from Russell et al. (2020) and are available at https://doi.org/10.5066/P9XT4WSP.\n",
    "The Python code used to compute the likelihoods are available in a demonstration notebook at https://github.com/thodson-usgs/objective-manuscript.\n",
    "\n",
    "## Disclaimer\n",
    "Any use of trade, firm, or product names is for descriptive purposes only and does not imply endorsement by the U.S. Government.\n",
    "\n",
    "## Acknowledgements\n",
    "The authors thank Hoshin V. Gupta for his encouragement and some inspiring discussion while waiting for his flight after HydroML 2022.\n",
    "Funding for this research was provided by the Hydro-terrestrial Earth Systems Testbed (HyTEST) project of the U.S. Geological Survey Integrated Water Prediction program.\n",
    "\n",
    "## References \n",
    "\n",
    "Akaike, H. (1974). A new look at the statistical model identification. IEEE Transactions on Automatic Control, 19 (6): 716–723. https://doi.org/10.1109/tac.1974.1100705\n",
    "\n",
    "Burnham, K.P. and Anderson, D.R. (2002). Model selection and multimodel inference: A Practical Information-Theoretic Approach. 2nd Edition, Springer-Verlag, New York.\n",
    "\n",
    "Cover, T.M., and Thomas, J. A. (2006). Elements of information theory, 2nd edition, John Wiley & Sons.\n",
    "\n",
    "Hodson, T.O. (2022). Root-mean-square error (RMSE) or mean absolute error (MAE): when to use them or not, Geosci. Model Dev., 15, 5481–5487. https://doi.org/10.5194/gmd-15-5481-2022\n",
    "\n",
    "Schwarz, G.E. (1978). Estimating the dimension of a model. Annals of Statistics, 6 (2), 461–464. https://doi.org/10.1214/aos/1176344136\n",
    "\n",
    "Smith, T., Sharma, A., Marshall, L., Mehrotra, R., and Sisson, S. (2010). Development of a formal likelihood function for improved Bayesian\n",
    "inference of ephemeral catchments, Water Resources Research, 46. https://doi.org/10.1029/2010wr009514\n",
    "\n",
    "Russell, A.M., Over, T.M., and Farmer, W.H. (2020). Cross-validation results for five statistical methods of daily streamflow estimation at 1,385 reference streamgages in the conterminous United States, Water Years 1981-2017: U.S. Geological Survey data release. https://doi.org/10.5066/P9XT4WSP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-pangeo",
   "language": "python",
   "name": "conda-env-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
